//trainer.py 
import collections
import copy
from dataclasses import asdict
import json
from pathlib import Path
import math 

import torch
from torch import nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, BatchSampler, RandomSampler

from torch.optim import Optimizer
from typing import Callable, Iterable, Tuple
from torch.distributions.bernoulli import Bernoulli

from tqdm import tqdm
from transformers import (
    Adafactor,
    AdamW,
    get_constant_schedule_with_warmup,
    get_linear_schedule_with_warmup,
)

from src.utils import data_utils, logging, metrics, model_utils
from src.utils.model_utils import device, to_device

logger = logging.get_logger(__name__)

#redefine AdamW
class ChildTuningAdamW(Optimizer):
    def __init__(
        self,
        params:Iterable[torch.nn.parameter.Parameter],
        lr: float = 1e-4,
        betas: Tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-6,
        weight_decay: float = 0.0,
        correct_bias: bool = True,
        reserve_p = 1.0,
        mode = None 
        ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)
        super().__init__(params, defaults)
        self.gradient_mask = None
        self.reserve_p = reserve_p
        self.mode = mode 

    def set_gradient_mask(self, gradient_mask):
        self.gradient_mask = gradient_mask


    @torch.no_grad()
    def step(self,closure: Callable = None):
        loss  = None 
        if closure is not None:
            with torch.enable_grad():
                loss =  closure()
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError("Adam does not support sparse gradients, please consider SparseAdam instead")

                # =================== HACK BEGIN =======================         
                
                if self.mode is not None:
                    if self.mode == 'ChildTuning-D':
                        if p in self.gradient_mask:
                            grad *= self.gradient_mask[p]
                    else: 
                        # ChildTuning-F
                        # 用fill_value(概率值)值填充grad的值,然后以概率fill_value采样1
                        grad_mask = Bernoulli(grad.new_full(size=grad.size(), fill_value=self.reserve_p))
                        grad *= grad_mask.sample() / self.reserve_p
                # =================== HACK END =======================

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state["step"] = 0
                    # Exponential moving average of gradient values
                    state["exp_avg"] = torch.zeros_like(p.data)
                    # Exponential moving average of squared gradient values
                    state["exp_avg_sq"] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
                beta1, beta2 = group["betas"]

                state["step"] += 1

                # Decay the first and second moment running average coefficient
                # In-place operations to update the averages at the same time
                exp_avg.mul_(beta1).add_(grad, alpha=1.0 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)
                denom = exp_avg_sq.sqrt().add_(group["eps"])

                step_size = group["lr"]
                if group["correct_bias"]:  # No bias correction for Bert
                    bias_correction1 = 1.0 - beta1 ** state["step"]
                    bias_correction2 = 1.0 - beta2 ** state["step"]
                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1

                p.data.addcdiv_(exp_avg, denom, value=-step_size)

                # Just adding the square of the weights to the loss function is *not*
                # the correct way of using L2 regularization/weight decay with Adam,
                # since that will interact with the m and v parameters in strange ways.
                #
                # Instead we want to decay the weights in a manner that doesn't interact
                # with the m/v parameters. This is equivalent to adding the square
                # of the weights to the loss with plain (non-momentum) SGD.
                # Add weight decay at the end (fixed version)
                p.data.add_(p.data, alpha=-group["lr"] * group["weight_decay"])

        return loss

def save(args, model, ckp, adapter=None):
    if args.save_every_best:
        name = f"model.{ckp}.pt"
    elif args.save_epoch_best:
        epoch = ckp.split(".")[0]
        name = f"model.{epoch}.pt"
    else:
        name = "model.pt"
    if adapter:
        p = Path(args.output_dir) / adapter
        if not p.exists():
            p.mkdir(parents=True)
        fn = Path(args.output_dir) / adapter / name
    else:
        fn = Path(args.output_dir) / name
    params = dict(list(model.named_parameters()))
    state_dict = collections.OrderedDict()
    for k, v in model.state_dict().items():
        if adapter and adapter not in k:
            continue
        if k in params and params[k].requires_grad:
            state_dict[k] = v
    logger.info(
        f"saving {adapter or 'model'} to {fn} "
        f"({len(state_dict)}/{len(params)} parameters)"
    )
    torch.save(state_dict, str(fn))


def asdict_(d):
    if type(d) == dict:
        return d
    return asdict(d)


class Trainer:
    def initialize(self, args):
        raise NotImplementedError

    def get_predictions(self, args, model, batch, outputs, tokenizer, **kwargs):
        raise NotImplementedError

    def get_inputs(self, args, batch, **kwargs):
        raise NotImplementedError

    def checkpoint(self, args, model, tokenizer, eval_datasets, ckp, best):
        report = self.evaluate(
            args,
            model,
            tokenizer,
            eval_datasets,
            ckp=ckp,
        )
        if args.criterion == "loss":
            compare = lambda a, b: a < b
        else:
            compare = lambda a, b: a > b

        if args.separate_adapter_checkpoints:
            new_best = False
            new_bests = []
            for a, a_best in best["experts"].items():
                a_report = report["experts"][a]
                a_new_best = False
                if compare(a_report[args.criterion], a_best[args.criterion]):
                    a_best["ckp"] = ckp
                    best["ckp"] = ckp
                    best["patience"] = 0
                    a_best.update(a_report)
                    logger.info(f"new best ({a}): {a_best}")
                    if args.save:
                        save(args, model, ckp, adapter=a)
                    new_best = True
                    a_new_best = True
                new_bests.append(a_new_best)

            if new_best:
                best["ckp"] = ckp
                best["patience"] = 0
                if any(new_bests) and args.separate_model_checkpoint:
                    save(args, model, ckp)
            else:
                best["patience"] += 1
        elif compare(report[args.criterion], best[args.criterion]):
            best["ckp"] = ckp
            best["patience"] = 0
            best.update(report)
            if len(args.train_on) == 1:
                log_report = {
                    k: v
                    for k, v in best.items()
                    if k not in args.train_on + ["experts"]
                }
            else:
                log_report = best
            logger.info(f"new best: {log_report}")
            if args.save:
                save(args, model, ckp)
        else:
            best["patience"] += 1
        stop_early = False
        if (
            args.patience not in (None, -1)
            and best["patience"] >= args.patience
        ):
            stop_early = True
            logger.info(
                f"{args.patience} checkpoints with no improvement," " stopping"
            )
        return best, stop_early, report

    def evaluate_one(
        self, args, model, tokenizer, dataset, eval_dataloader, ckp=""
    ):
        model.eval()
        logger.info(f"evaluating on {dataset}")
        with torch.no_grad():  #测试模式
            t = tqdm(eval_dataloader, desc=f"eval [{ckp}]")
            predictions = []
            eval_loss = 0
            for step, batch in enumerate(t):
                inputs = self.get_inputs(args, batch)
                to_device(inputs)
                inputs["is_prediction"] = True
                outputs = model(**inputs)
                predictions += self.get_predictions(
                    args, model, batch, outputs, tokenizer
                )
                loss = outputs.loss.mean()
                eval_loss += loss.item()
                postfix = {"loss": loss.item()}
                if outputs.details and "kl" in outputs.details:
                    kl = outputs.details["kl"].item()
                    postfix["kl"] = kl
                t.set_postfix(postfix)
            eval_loss = eval_loss / len(eval_dataloader)
            logger.info(f"avg eval loss: {eval_loss}")
        report, predictions = metrics.score_predictions(predictions)
        logger.info(f"{dataset} results at {ckp}: {report}")
        logger.info(f"writing results to {args.output_dir}")
        with open(
            Path(args.output_dir) / f"metrics.{dataset}.{ckp}.json", "w"
        ) as f:
            json.dump(report, f, indent=2)
        pckp = f"{ckp}." if (ckp and not ckp[0].isdigit()) else ""
        with open(
            Path(args.output_dir) / f"predictions.{dataset}.{pckp}json", "w"
        ) as f:
            json.dump(predictions, f, indent=2)
        return report

    def evaluate(self, args, model, tokenizer, eval_datasets, ckp=""):
        logger.info("evaluating")
        eval_dataloaders = {}
        for dataset in args.eval_on:
            eval_dataloader = DataLoader(
                eval_datasets[dataset],
                batch_size=args.eval_batch_size,
                collate_fn=data_utils.collate,
            )
            eval_dataloaders[dataset] = eval_dataloader
        reports = {}
        for dataset, dataloader in eval_dataloaders.items():
            reports[dataset] = self.evaluate_one(
                args, model, tokenizer, dataset, dataloader, ckp
            )
        report = metrics.average_dicts(list(reports.values()), short=True)
        logger.info(
            f"average eval {args.criterion} at {ckp}: {report[args.criterion]}"
        )
        report.update(reports)
        logger.info(f"writing results to {args.output_dir}")
        with open(Path(args.output_dir) / f"metrics.{ckp}.json", "w") as f:
            json.dump(report, f, indent=2)
        return report

    def load_from(self, args, path_or_paths, model, **kwargs):
        paths = (
            path_or_paths if type(path_or_paths) == list else [path_or_paths]
        )
        state_dict = {}
        for path in paths:
            if str(path).endswith(".pt"):
                fn = path
            else:
                fns = sorted(
                    Path(path).glob("model*"),
                    key=lambda p: p.lstat().st_ctime,
                )
                if len(fns) == 0:
                    raise ValueError(f"no model.pt in {path}")
                fn = fns[-1]
            logger.info(f"loading checkpoint from {fn}")
            p_state_dict = torch.load(fn, map_location=model_utils.device())
            path_name = Path(path).name
            # rename head parameters to heads.{path_name}
            remap_adapters = (
                args.made
                or args.average_adapters
                or args.weighted_average_before_training
            )
            if remap_adapters and path_name in args.adapter_names:
                remapped = {}
                for k, p in p_state_dict.items():  #外部加载的head.weight->head.path_name.weight
                    if "head." in k:
                        rk = k.replace("head.", f"heads.{path_name}.")
                        remapped[rk] = p
                logger.info(
                    f"remapping {len(remapped)} adapter parameters from {path}"
                )
                p_state_dict.update(remapped)
            state_dict.update(p_state_dict) 

        if args.average_adapters:
            logger.info(f"averaging adapters")
            proportions = None
            avg_dict = model_utils.average_adapter_params(
                args, state_dict, proportions=proportions
            )
            logger.info(f"averaging {len(avg_dict)} parameters")
            state_dict.update(avg_dict)
        # model从state_dict中加载属于自己结构的权重
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        logger.info(f"{len(missing)} missing, {len(unexpected)} unexpected")
        logger.info(f"missing: {missing}")
        logger.info(f"unexpected: {unexpected}")
        missing = set(missing)
        missing_new = [
            k
            for k, p in model.named_parameters()
            if p.requires_grad and k in missing
        ]
        logger.info(f"missing parameters with requires_grad: {missing_new}")
        return model

    def set_frozen(self, args, model):
        total = frozen = 0
        for k, p in model.named_parameters(): 
            total += 1
            if model_utils.freeze(args, k):
                p.requires_grad = False
                frozen += 1
            else:
                p.requires_grad = True  # 只有adapter 和 head 模块更新
        logger.info(f"froze {frozen}/{total} parameters")
    def train(self, args, model, tokenizer, train_dataset, eval_datasets):
        logger.info(f"writing args to {args.output_dir}")
        with open(Path(args.output_dir) / "args.json", "w") as f:
            json.dump(vars(args), f, indent=2)

        if len(args.train_on) > 1: 
            sampler = data_utils.DatasetMixerSampler(
                train_dataset,
                batch_size=args.train_batch_size,
            )
        elif args.bucket_sampler:
            sampler = data_utils.BucketSampler(
                train_dataset, batch_size=args.train_batch_size
            )
        else:
            sampler = BatchSampler(
                RandomSampler(train_dataset),
                args.train_batch_size,
                drop_last=False,
            )

        train_dataloader = DataLoader(
            train_dataset,
            batch_sampler=sampler,
            collate_fn=data_utils.collate,
        )

        model.to(device())

        self.set_frozen(args, model)  #模型阶段

        no_decay = ["bias", "LayerNorm.weight"]

        is_adapter_param = lambda k: ".adapters." in k or "head" in k
        
        if args.adapter or args.made and args.adapter_learning_rate:
            adapter_param_names, adapter_params = zip(
                *[
                    (k, p)
                    for k, p in model.named_parameters()
                    if p.requires_grad and is_adapter_param(k) and not any(nd in k for nd in no_decay)
                ]
            )
            base_params = [
                p
                for k, p in model.named_parameters()
                if p.requires_grad and not is_adapter_param(k) and not any(nd in k for nd in no_decay)
            ]
            
            base_params_bias_LayerNorm=[
             p
                for k, p in model.named_parameters() if any(nd in k for nd in no_decay)
            ]

            params = [
                # {"params": base_params,  "weight_decay": args.weight_decay },
                # {"params": base_params_bias_LayerNorm,  "weight_decay": 0.0 },
                # {"params": adapter_params, "lr": args.adapter_learning_rate,"weight_decay": args.weight_decay},
                {"params": base_params},
                {"params": base_params_bias_LayerNorm,  "weight_decay": 0.0 },
                {"params": adapter_params, "lr": args.adapter_learning_rate},
            ]

            print(
                f"setting lr to {args.adapter_learning_rate} for "
                f"{len(adapter_param_names)} adapter params"
            )
        else:
            params = [p for p in model.parameters() if p.requires_grad]

        if args.optimizer == "adafactor":
            optimizer = Adafactor(
                params,
                scale_parameter=False,
                relative_step=False,
                warmup_init=False,
                lr=args.learning_rate,
            )
        else:   #对这个地方进行重写
            # optimizer = AdamW(
            #     params,
            #     lr=args.learning_rate,
            #     weight_decay=args.weight_decay,
            # )

            optimizer_cls = ChildTuningAdamW
            optimizer =optimizer_cls( 
                params,
                lr=args.learning_rate,
                weight_decay=args.weight_decay,
                reserve_p=args.reserve_p,
                mode='ChildTuning-F'
                )
        scheduler = None
        steps_per_epoch = (
            len(train_dataloader) / args.gradient_accumulation_steps
        )
        num_steps = max(args.steps, args.epochs * steps_per_epoch)#修改这里的steps
        if args.steps > (args.epochs * steps_per_epoch):
            args.epochs = int(args.steps // steps_per_epoch) + 1
        logger.info(f"training for {num_steps} steps / {args.epochs} epochs")
        warmup_steps = args.warmup_steps or args.warmup_ratio * num_steps
        if args.scheduler == "linear":
            logger.info(
                f"using linear lr schedule,  num_steps: {num_steps}, "
                f"warmup: {warmup_steps}"
            )
            scheduler = get_linear_schedule_with_warmup(
                optimizer=optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=args.epochs * len(train_dataloader),
            )
        elif args.scheduler == "constant":
            logger.info(
                f"using constant lr schedule,  num_steps: {num_steps}, "
                f"warmup: {warmup_steps}"
            )
            scheduler = get_constant_schedule_with_warmup(
                optimizer=optimizer,
                num_warmup_steps=warmup_steps,
            )

        lr = args.learning_rate

        best = {
            "ckp": "",
            "em": 0,
            "f1": 0,
            "loss": float("inf"),
            "patience": 0,
            "lr": lr,
        }
        if args.criterion not in best:
            raise NotImplementedError(f"unknown criterion {args.criterion}")
        if args.separate_adapter_checkpoints:
            copies = [copy.copy(best) for _ in args.adapter_names]
            best["experts"] = {a: c for a, c in zip(args.adapter_names, copies)}

        stop_early = False
        global_step = 0

        if args.eval_before_training:#在训练之前进行测试，找到每个adapter的权重
            logger.info(f"evaluating before training")
            best, _, _ = self.checkpoint(
                args,
                model,
                tokenizer,
                eval_datasets,
                ckp="0.0",
                best=best,
            )

        for epoch in range(args.epochs):
            epoch_loss = 0
            checkpoint_loss = 0
            logger.info(f"epoch: {epoch}")
            t = tqdm(train_dataloader, desc=f"train [{epoch}]")
            for step, batch in enumerate(t):
                model.train()  #  input_ids (batch_size,len)  #  start (batch_size) end (batch_size)
                # attention_mask (batch_size,len)
                inputs = self.get_inputs(args, batch)#关注inputs的数据结构即可
                to_device(inputs)
                outputs = model(**inputs)
                loss = outputs.loss.mean() / args.gradient_accumulation_steps
                epoch_loss += loss.item()
                checkpoint_loss += loss.item()
                loss.backward()
                postfix = {"loss": loss.item()}
                if outputs.details and "kl" in outputs.details:
                    kl = outputs.details["kl"].item()
                    postfix["kl"] = kl
                t.set_postfix(postfix)

                if loss > 1000:
                    logger.warning(f"bad loss")
                    logger.info(f"{batch}")

                if step > 0 and step % args.gradient_accumulation_steps == 0:
                    global_step += 1
                    if args.max_grad_norm:
                        nn.utils.clip_grad_norm_(
                            model.parameters(),
                            args.max_grad_norm
                        )
                    optimizer.step()    #对这个地方进行改进
                    if scheduler is not None:
                        scheduler.step()
                    model.zero_grad()
                    optimizer.zero_grad()

                if step > 0 and step % args.eval_every == 0:
                    ckp = f"{epoch}.{int((epoch * steps_per_epoch) + step)}"
                    checkpoint_loss = checkpoint_loss / args.eval_every
                    logger.info(f"training loss (ckp {ckp}): {checkpoint_loss}")
                    checkpoint_loss = 0
                    lr = optimizer.param_groups[0]["lr"]
                    best["lr"] = lr
                    best, stop_early, report = self.checkpoint(
                        args, model, tokenizer, eval_datasets, ckp, best
                    )
                    if stop_early:
                        break
                    if args.dynamic_sampling:
                        seen_examples = (
                            global_step
                            * args.train_batch_size
                            * args.gradient_accumulation_steps
                        )
                        if seen_examples > args.dynamic_sampling_after:
                            sampler = train_dataloader.batch_sampler
                            sampler.set_dynamic_sampling_weights(report)

                if global_step > num_steps:  #修改这里让程序终止
                    stop_early = True
                    break

            if stop_early:
                break

            logger.info(f"end of epoch {epoch}")
            epoch_loss /= len(train_dataloader)
            logger.info(f"average training loss: {epoch_loss}")
            ckp = f"{epoch}.{(epoch + 1) * len(train_dataloader)}"
            best, stop_early, report = self.checkpoint(
                args, model, tokenizer, eval_datasets, ckp, best
            )
            if stop_early:
                break
        return best
